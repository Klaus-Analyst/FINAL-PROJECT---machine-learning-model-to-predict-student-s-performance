# -*- coding: utf-8 -*-
"""finalproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wt1nkXkxjcfp7MD1cc6nUZFwEMUX1uEp

## Problem Framing & Big Picture
### 1. Problem and objective in business terms and how the solution will be used.
In this project, we aim to address a critical challenge within the Portuguese school system by developing a machine learning model to predict students' final grades (G3) without the input of their first (G1) and second (G2) term grades. This approach focuses on early identification of students who may require additional support and interventions to improve their academic performance. By analyzing a variety of factors, including demographic, social, and academic attributes, the model will serve as a proactive tool for the school's advising team. The objective is to enhance the effectiveness and efficiency of educational support, aiding in resource allocation by prioritizing assistance for students most in need. The implementation of this predictive model promises to improve student retention rates, boost the school system's reputation through enhanced academic outcomes, and lead to better overall student success. This data-driven approach aims to transform educational assistance, making it more targeted and equitable, and ultimately fostering a positive impact on the community and higher education.

### 2. Problem Framing

This project is best framed as a supervised learning problem that operates in an offline environment. Supervised learning is a type of machine learning where the model is trained on a labeled dataset, which means that each example in the training set is accompanied by the correct output. In the context of predicting student performance, the model learns from historical data that includes both the features (such as demographic and academic attributes) and the target variable (the final grade, G3). This approach allows the model to learn the relationships between the features and the target, enabling it to make predictions for new, unseen data. On the other hand, unsupervised learning involves working with datasets without labeled outcomes, focusing on identifying patterns or structures within the data. The problem is also categorized as an offline task, meaning the model is trained on a fixed dataset and does not continuously update its learning with new data in real-time, unlike online learning where models learn incrementally as new data arrives. This offline approach is suitable for our project's scope, given that the data consists of historical student performance records, allowing for comprehensive model training and evaluation before deployment.

### 3. Machine Learning Approach (Classification)

For this project, we will focus on a classification task as the specific machine learning approach to predict whether a student passes or fails based on their final G3 grade. Classification involves categorizing data into predefined groups or classes. In our context, this means determining if a student's performance will result in a pass or fail outcome, simplifying the target into a binary choice that directly aligns with the need for intervention. This approach is chosen over regression, which predicts a continuous quantity (the exact final grade), because it more directly addresses the business problem of identifying students who require additional support. By classifying students into "pass" or "fail" categories, the school can efficiently allocate resources and support to those most in need, thereby preventing failures and improving overall student success rates. This method simplifies the decision-making process for interventions and is particularly effective in educational settings where the primary concern is ensuring students achieve a minimum level of proficiency.

### 4. Metrics for measuring model performance
To measure the performance of our classification model, we will primarily use accuracy, precision, recall, and the F1 score as our metrics. Accuracy indicates the proportion of total predictions our model gets right, combining both pass and fail predictions. However, accuracy alone might not give a complete picture, especially if our data is imbalanced (e.g., more pass cases than fail). Precision measures the proportion of positive identifications (students predicted to fail) that were actually correct, which is crucial for ensuring that interventions are not mistakenly targeted. Recall, or sensitivity, assesses the proportion of actual fails that were correctly identified, highlighting the model's ability to catch all at-risk students. The F1 score harmonizes precision and recall into a single metric, balancing the trade-off between the two and providing a comprehensive measure of the model's performance in cases where an equal importance is placed on both precision and recall. These metrics together will offer a holistic view of our model’s effectiveness in identifying students in need of support, ensuring that our interventions are both accurate and equitable.

## Get the data

## 1. Import data
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Load the dataset
data = '/content/student-mat.csv'
df = pd.read_csv(data)

"""### 2. The size and type of data"""

#Size of the dataset
print("Dataset size (rows, columns):", df.shape)

# Data types of each column
print("Data types of each column:\n", df.dtypes)

"""## 3. Available features and description

*   school: The student's school (GP - Gabriel Pereira or MS - Mousinho da Silveira).
*   sex: The student's sex (F - female or M - male).
*   age: The student's age (numeric: from 15 to 22).
*   address: The student's home address type (U - urban or R - rural).
*   famsize: Family size (LE3 - less or equal to 3 or GT3 - greater than 3).
*  Pstatus: Parent's cohabitation status (T - living together or A - apart).
*   Medu: Mother's education (0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education, 4 – higher education).
*   Fedu: Father's education (0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education, 4 – higher education).
*   Mjob: Mother's job (teacher, health care related, civil services (e.g., administrative or police), at_home, or other).
*   Fjob: Father's job (teacher, health care related, civil services (e.g., administrative or police), at_home, or other).
*   reason: Reason to choose this school (close to home, school reputation, course preference, or other).
*   guardian: Student's guardian (mother, father, or other).
*   traveltime: Home to school travel time (1 - <15 min, 2 - 15 to 30 min, 3 - 30 min. to 1 hour, 4 - >1 hour).
*   studytime: Weekly study time (1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours).
*   failures: Number of past class failures (n if 1<=n<3, else 4).
*   schoolsup: Extra educational support (yes or no).
*   famsup: Family educational support (yes or no).


*   paid: Extra paid classes within the course subject (Math or Portuguese) (yes or no).

*   activities: Extra-curricular activities (yes or no).
*   nursery: Attended nursery school (yes or no).


*   higher: Wants to take higher education (yes or no).


*   internet: Internet access at home (yes or no).

*   romantic: In a romantic relationship (yes or no).
*   famrel: Quality of family relationships (from 1 - very bad to 5 - excellent).

*   freetime: Free time after school (from 1 - very low to 5 - very high).
*   goout: Going out with friends (from 1 - very low to 5 - very high).
*   rDalc: Workday alcohol consumption (from 1 - very low to 5 - very high).
*   rWalc: Weekend alcohol consumption (from 1 - very low to 5 - very high).
*   health: Current health status (from 1 - very bad to 5 - very good).
*   absences_G1, absences_G2, absences_G3: Number of school absences for G1,G2, and G3 terms (numeric).
*   G1, G2, G3: First term, second term, and final grade (numeric: from 0 to 20).

### Target Variable
The target variable is G3, which will be used to determine whether a student passes or fails
"""

# Transform 'G3' into a binary variable for classification ('Pass' column: 1 for pass, 0 for fail)
df['Pass'] = (df['G3'] >= 10).astype(int)

# Define the features (X) and the target variable (y)
X = df.drop(['G3', 'Pass'], axis=1)  # Excludes the 'G3' column and the newly created 'Pass' column
y = df['Pass']  # Target variable

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# This sets aside 20% of the data for testing and uses a random state for reproducibility

"""## Explore the data

### Training set attributes and their characteristics
"""

# Descriptive statistics for numerical features
print("Descriptive Statistics for Numerical Features:")
print(X_train.describe())

# Descriptive statistics for categorical features
print("\nDescriptive Statistics for Categorical Features:")
print(X_train.describe(include=['object']))


# Merging X_train with y_train for visualization
train_dataset = X_train.copy()
train_dataset['Pass'] = y_train

# Box plots for numerical features against the target variable
# Replace 'numerical_feature' with the actual names of your numerical features
for column in ['age', 'Medu', 'Fedu', 'studytime']:
    if column in train_dataset.columns:
        plt.figure(figsize=(10,6))
        sns.boxplot(x='Pass', y=column, data=train_dataset)
        plt.title(f'Relationship between Pass and {column}')
        plt.show()

# For categorical features, stacked bar charts can be useful
# This is a bit more complex to create directly without specific feature selection

sns.histplot(data=train_dataset, x='age', hue='Pass', multiple='stack', kde=True)
plt.title('Age Distribution by Pass/Fail Status')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

sns.boxplot(data=train_dataset, x='Pass', y='studytime')
plt.title('Study Time by Pass/Fail Status')
plt.xlabel('Pass/Fail')
plt.ylabel('Study Time')
plt.show()

import numpy as np
plt.figure(figsize=(10, 8))
sns.heatmap(train_dataset.select_dtypes(include=[np.number]).corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

sns.histplot(data=df, x='G3', bins=20, kde=True)
plt.title('Distribution of Final Grades')
plt.xlabel('Final Grade (G3)')
plt.ylabel('Number of Students')
plt.show()

sns.countplot(data=df, x='school')
plt.title('Student Count by School')
plt.xlabel('School')
plt.ylabel('Number of Students')
plt.show()

sns.countplot(data=df, x='school')
plt.title('Student Count by School')
plt.xlabel('School')
plt.ylabel('Number of Students')
plt.show()

"""## Prepare the data"""


class FeatureSelector(BaseEstimator, TransformerMixin):
    def __init__(self, feature_names):
        # feature_names is a list of names of features to keep
        self.feature_names = feature_names

    def fit(self, X, y=None):
        # Nothing to do here
        return self

    def transform(self, X, y=None):
        # Return a new DataFrame with only the selected features
        return X[self.feature_names]

# Example usage:
feature_names = ['studytime', 'failures', 'Medu', 'Fedu', 'Dalc', 'Walc', 'absences_G1', 'absences_G2', 'absences_G3']
# Initialize the transformer with the names of features you've decided to keep
selector = FeatureSelector(feature_names=feature_names)

# Transform the training data
X_train_selected = selector.transform(X_train)

# Later, you can apply the same transformation to your test data
X_test_selected = selector.transform(X_test)


# Numerical features to scale
numerical_features = ['age', 'Medu', 'Fedu', 'studytime', 'failures', 'Dalc', 'Walc', 'absences_G1', 'absences_G2', 'absences_G3']
# Categorical features to encode
categorical_features = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian']

# Creating transformers
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combining transformers into a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Full pipeline: preprocessing + modeling (example with a classifier)


pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', RandomForestClassifier(random_state=42))])


# Adjusting the numerical_transformer to fill in missing values for numerical features
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Filling missing values with the mean for numerical features
    ('scaler', StandardScaler())
])

# Adjusting the categorical_transformer to fill in missing values for categorical features
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Filling missing values with the most frequent value for categorical features
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# The rest of the pipeline remains the same
# Combining transformers into a ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Full pipeline: preprocessing + modeling (example with a classifier)


pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('classifier', RandomForestClassifier(random_state=42))])



class AbsenceTransformer(BaseEstimator, TransformerMixin):
    def __init__(self, drop_G1_G2=True):
        self.drop_G1_G2 = drop_G1_G2

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        X = X.copy()
        X['total_absences'] = X['absences_G1'] + X['absences_G2'] + X['absences_G3']
        X.drop(['absences_G1', 'absences_G2', 'absences_G3'], axis=1, inplace=True)

        if self.drop_G1_G2:
            X.drop(['G1', 'G2'], axis=1, inplace=True)

        return X

# Incorporating the custom transformer into the preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features),
        ('absence_transform', AbsenceTransformer(drop_G1_G2=True), ['absences_G1', 'absences_G2', 'absences_G3', 'G1', 'G2'])
    ], remainder='passthrough')

# Adjusting the numerical_transformer within the pipeline to include feature scaling
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Imputing missing values with the mean
    ('scaler', StandardScaler())  # Scaling the features
])


# Identifying binary/ordinal features and nominal features
binary_ordinal_features = ['Pstatus']  # Example: add more as needed
nominal_features = ['school', 'sex', 'address', 'famsize', 'Mjob', 'Fjob', 'reason', 'guardian']  # Example: adjust based on your data

# Creating transformers for binary/ordinal and nominal features
binary_ordinal_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ordinal', OrdinalEncoder())
])

nominal_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Adjusting the ColumnTransformer to include these transformations
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('binary_ordinal', binary_ordinal_transformer, binary_ordinal_features),
        ('nominal', nominal_transformer, nominal_features),
        ('absence_transform', AbsenceTransformer(drop_G1_G2=True), ['absences_G1', 'absences_G2', 'absences_G3', 'G1', 'G2'])
    ], remainder='passthrough')

# Assuming the preprocessor is the ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('binary_ordinal', binary_ordinal_transformer, binary_ordinal_features),
        ('nominal', nominal_transformer, nominal_features),
        # Ensure the custom absence_transform is properly integrated if needed
    ], remainder='passthrough')

# Applying the pipeline to transform the training data with and without G1/G2 columns
# First, ensure your ColumnTransformer includes the custom transformer with a parameter to control dropping G1/G2

# Pipeline without dropping G1/G2
pipeline_with_G1_G2 = Pipeline(steps=[
    ('preprocessor', preprocessor),
])

# Adjusting the absence_transformer within the preprocessor for this scenario
preprocessor_with_G1_G2 = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('binary_ordinal', binary_ordinal_transformer, binary_ordinal_features),
        ('nominal', nominal_transformer, nominal_features),
        ('absence_transform', AbsenceTransformer(drop_G1_G2=False), ['absences_G1', 'absences_G2', 'absences_G3', 'G1', 'G2'])
    ], remainder='passthrough')

# Pipeline with dropping G1/G2
pipeline_without_G1_G2 = Pipeline(steps=[
    ('preprocessor', preprocessor),
])

# Using the original preprocessor configuration for this scenario

# Transforming the training data
X_train_transformed_with_G1_G2 = pipeline_with_G1_G2.fit_transform(X_train)
X_train_transformed_without_G1_G2 = pipeline_without_G1_G2.fit_transform(X_train)

# Output the shape of the two transformed training sets
print("Shape with G1/G2 columns:", X_train_transformed_with_G1_G2.shape)
print("Shape without G1/G2 columns:", X_train_transformed_without_G1_G2.shape)

"""## Shortlisting Promising Models

"""